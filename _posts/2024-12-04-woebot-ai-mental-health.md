---
layout: post
title: "Woebot and the Mental Health Crisis: What Can AI Really Do?"
subtitle: "Exploring the ethical and practical challenges of AI-driven therapy tools"
date: 2024-12-04
author: Adam Kearsey
tags: [AI, mental health, ethics, technology, Woebot]
---  

As the mental health crisis deepens, artificial intelligence tools like *Woebot* are stepping up to fill the gap. These tools promise round-the-clock, anonymous, and accessible support for those facing anxiety, depression, and stress. That kind of access matters, especially when cost and availability keep many from getting the help they need.

But despite the hype, the role of AI in mental health support isn’t straightforward. It brings real benefits, but also serious limitations and ethical risks that need urgent attention.

<img src="/assets/img/Therapist.png" alt="Therapist" style="max-width: 250px; height: auto; display: block; margin-bottom: 0.5em;" />

## Why AI Is Gaining Ground

Mental health services are stretched thin. As Karen Brown (2021) noted, “almost all psychologists and academics agree … there is not enough affordable mental health care for everyone who needs it.” AI offers one way to close this gap.

Tools like *Woebot* are free, available 24/7, and easy to use. Users don’t need to book appointments or worry about being judged — they can just talk. On the provider side, AI reduces the workload by improving diagnostic consistency, speeding up documentation, and even helping with training (Chenneville et al., 2024). In this way, AI doesn’t just expand access; it boosts efficiency too.

## Where It Falls Short

But digital tools have limits, especially when it comes to something as personal as therapy. Mental health care depends on empathy, trust, and deep human connection — things that AI can only simulate, not truly provide (Ho & Perry, 2023).

As Baeza-Yates et al. (2024) point out, current systems lack true semantic understanding. Chatbots can’t interpret non-verbal cues, deal with transference, or navigate the power dynamics that trained therapists manage (Ho & Perry, 2023). They might feel responsive, but the conversation stays surface-level.

There are safety issues too. Tools like Woebot have responded poorly when users express suicidal thoughts (Chenneville et al., 2024). Without the ability to detect risk or adapt to cultural context, the potential for harm is real.

Worse, users can grow emotionally attached to these tools, only to lose access because of subscription costs or app changes. That kind of emotional disruption can leave users worse off (Tavory, 2024).

## The Ethical Gaps

Mental health AI operates in a regulatory grey area. Apps like Woebot often market themselves as “self-help” tools, not healthcare providers (Brown, 2021). That means they’re not subject to the same ethical rules or professional standards.

This creates real problems: unclear accountability, weak data privacy protections, lack of transparency around how models are trained, and biased outputs (Chenneville et al., 2024). Many models reflect the same systemic biases found in society, including racism, sexism, and heterosexism (Chenneville et al., 2024, p. 163).

>  *“Therapy has enough problems on its own, and now they've brought all of the problems of algorithmic technology to bear."  
-Dr. Hannah Zevin (Brown, 2021)*

Without stronger oversight, these tools risk undermining the core values of mental health care, like dignity, beneficence, and trust (Chenneville et al., 2024).

## Where Do We Go From Here?

To make AI tools safer and more responsible, several things need to happen:

- Users must be clearly informed when AI is being used
- Psychologists should receive training in AI ethics
- Policies must be developed to govern how generative AI is used
- Researchers and professionals need to keep updating ethical frameworks as the technology evolves

These steps won’t solve everything, but they’re essential for making AI more useful and less harmful in mental health care.

## Final Thoughts

AI tools like *Woebot* show promise. They offer support to people who might not otherwise get it. But they aren’t a replacement for human care.
To make sure AI adds value instead of creating harm, we need to be clear about what these tools can do, what they can’t, and how they should be used. That means prioritizing transparency, responsibility, and ethical design from the start.  
– Adam Kearsey

## References

1. Baeza-Yates, R., Fayyad, U. M., & Fayyad, U. (2024). *Responsible AI: An Urgent Mandate.* IEEE Intelligent Systems, 39(1), 12–17. [https://doi.org/10.1109/MIS.2023.3343488](https://doi.org/10.1109/MIS.2023.3343488)
2. Brown, K. (2021). *Something Bothering You? Tell It to Woebot.* International New York Times, 10 June 2021. [https://www.nytimes.com/2021/06/01/health/artificial-intelligence-therapy-woebot.html](https://www.nytimes.com/2021/06/01/health/artificial-intelligence-therapy-woebot.html)  
3. Chenneville, T., Duncan, B., & Silva, G. (2024). *More questions than answers: Ethical considerations at the intersection of psychology and generative artificial intelligence.* Translational Issues in Psychological Science, 10(2), 162–178. [https://doi.org/10.1037/tps0000400](https://doi.org/10.1037/tps0000400)  
4. Carter, K. (2024). *Mental Health and AI.* Bulletin of the American Academy of Arts and Sciences, 77(3), 12–13. [https://www.jstor.org/stable/27304418](https://www.jstor.org/stable/27304418)  
5. Ho, A., & Perry, J. (2023). *What We Owe Those Who Chat Woe: A Relational Lens for Mental Health Apps.* American Journal of Bioethics, 23(10), 77–80. [https://doi.org/10.1080/15265161.2023.2250306](https://doi.org/10.1080/15265161.2023.2250306)
6. Tavory, T. (2024). *Regulating AI in Mental Health: Ethics of Care Perspective.* JMIR Mental Health; 11: e58493. [https://mental.jmir.org/2024/1/e58493](https://mental.jmir.org/2024/1/e58493)  